{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Jupyter file will guide you to go through an interesting experiment -- using GNN to achieve a BIM node classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "# torch and dgl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from dgl.data.utils import load_graphs\n",
    "# basic machine learning libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "# self construct functions\n",
    "from node_evaluation import collate, evalEdge \n",
    "from SAGEE import SAGEE\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "device = torch.device('cpu') # CPU is enough for processing small graphs\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set basic parameters. The default runing epoch is 200. You can play with different hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "batch_size = 1\n",
    "n_classes = 9 # nine room classes here\n",
    "weight_decay=5e-4\n",
    "num_channels = 50\n",
    "lr = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load RoomGraph dataset.\n",
    "\n",
    "RoomGraph is a self-designed graph dataset containing 224 apartment layouts collecting from 3 countries. \n",
    "\n",
    "RoomGraph has 9 different node classes, and each node and edge owns its feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset 161, val dataset 18, test dataset 45\n"
     ]
    }
   ],
   "source": [
    "bg = load_graphs(\"./../dataset/roomgraph.bin\")[0]\n",
    "\n",
    "# data split\n",
    "trainvalid, test_dataset =  train_test_split(bg, test_size=0.2, random_state=42)\n",
    "train_dataset, valid_dataset = train_test_split(trainvalid, test_size=0.1, random_state=42)\n",
    "\n",
    "# data batch for parallel computation\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, collate_fn=collate)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate)\n",
    "\n",
    "print(\"train dataset %i, val dataset %i, test dataset %i\"%(len(train_dataset), \\\n",
    "    len(valid_dataset), len(test_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SAGE-E model. \n",
    "\n",
    "SAGE-E is an improved algorithm based on [GraphSAGE](https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf). \n",
    "\n",
    "The main improvement is that SAGE-E can leverage both node and edge features, but GraphSAGE can only learn from node features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAGEE(\n",
      "  (layers): ModuleList(\n",
      "    (0): SAGEELayer(\n",
      "      (W_msg): Linear(in_features=13, out_features=50, bias=True)\n",
      "      (W_apply): Linear(in_features=58, out_features=50, bias=True)\n",
      "    )\n",
      "    (1): SAGEELayer(\n",
      "      (W_msg): Linear(in_features=55, out_features=50, bias=True)\n",
      "      (W_apply): Linear(in_features=100, out_features=50, bias=True)\n",
      "    )\n",
      "    (2): SAGEELayer(\n",
      "      (W_msg): Linear(in_features=55, out_features=25, bias=True)\n",
      "      (W_apply): Linear(in_features=75, out_features=25, bias=True)\n",
      "    )\n",
      "    (3): SAGEELayer(\n",
      "      (W_msg): Linear(in_features=30, out_features=9, bias=True)\n",
      "      (W_apply): Linear(in_features=34, out_features=9, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model loading \n",
    "ndim_in = train_dataset[0].ndata['feat'].shape[1]\n",
    "edim_in = train_dataset[0].edata['relation'].shape[1]\n",
    "\n",
    "model = SAGEE(ndim_in, n_classes, edim_in,  F.relu, 0.2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 train | Accuracy: 0.3210 | Loss: 1.8735\n",
      "Validation | Accuracy: 0.4354 | Loss: 1.6016\n",
      "\n",
      "Epoch 002 train | Accuracy: 0.4663 | Loss: 1.5542\n",
      "Validation | Accuracy: 0.5122 | Loss: 1.3558\n",
      "\n",
      "Finish training! Using 4.7578 s:\n"
     ]
    }
   ],
   "source": [
    "train_acc_all, train_loss_all  = [], []\n",
    "val_acc_all, val_loss_all = [], []\n",
    "\n",
    "\n",
    "train_startime = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    #### train one epoch \n",
    "    model.train()\n",
    "\n",
    "    train_acc_list = []\n",
    "    train_loss_list = []\n",
    "\n",
    "    # feed graph to algorithm one by one\n",
    "    for batch, subgraph in enumerate(train_dataloader):\n",
    "\n",
    "        subgraph = subgraph.to(device) \n",
    "        nfeat = subgraph.ndata['feat'].float()\n",
    "        efeat = subgraph.edata['relation'].float()\n",
    "\n",
    "        logits = model(subgraph, nfeat, efeat) # get the prediction from models \n",
    "\n",
    "        # calculate the accuracy \n",
    "        gt = torch.argmax(subgraph.ndata['label'], dim=1) # ground true labels\n",
    "        pre  = torch.argmax(logits, dim=1)  # prediction labels \n",
    "        correct = torch.sum(pre == gt) # calculate the right labels \n",
    "\n",
    "        acc = correct.item()*1.0/len(gt) # calculate the accuracy \n",
    "        train_acc_list.append(acc) \n",
    "\n",
    "        # compute the loss\n",
    "        loss = F.cross_entropy(logits, gt) # using cross entropy \n",
    "        train_loss_list.append(loss.item()) \n",
    "\n",
    "        # backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # calculate acc and loss for each epoch\n",
    "    train_loss_epoch =  np.array(train_loss_list).mean()\n",
    "    train_acc_epoch =  np.array(train_acc_list).mean()\n",
    "\n",
    "    train_loss_all.append(train_loss_epoch)\n",
    "    train_acc_all.append(train_acc_epoch)\n",
    "\n",
    "    print(\"Epoch {:03d} train | Accuracy: {:.4f} | Loss: {:.4f}\".format(\\\n",
    "        epoch+1, train_acc_epoch, train_loss_epoch))\n",
    "\n",
    "\n",
    "    #### start evaluation\n",
    "    val_acc_list, val_loss_list = [], []\n",
    "\n",
    "    for batch, subgraph in enumerate(valid_dataloader):\n",
    "        subgraph = subgraph.to(device)\n",
    "\n",
    "        # calculate the accuracy and loss\n",
    "        nfeat = subgraph.ndata['feat'].float()\n",
    "        efeat = subgraph.edata['relation'].float()\n",
    "\n",
    "        acc, loss, _, _, _, _ = evalEdge(model, nfeat, efeat, subgraph, subgraph.ndata['label'], n_classes)\n",
    "\n",
    "        # obtain acc and loss\n",
    "        val_acc_list.append(acc)\n",
    "        val_loss_list.append(loss.item())\n",
    "\n",
    "    # calculate the loss and acc for all graphs in one epoch\n",
    "    val_loss_epoch =  np.array(val_loss_list).mean()\n",
    "    val_acc_epoch =  np.array(val_acc_list).mean()\n",
    "\n",
    "    # append for drawing the curs\n",
    "    val_acc_all.append(val_acc_epoch)\n",
    "    val_loss_all.append(val_loss_epoch)\n",
    "        \n",
    "    print(\"Validation | Accuracy: {:.4f} | Loss: {:.4f}\\n\".format(val_acc_epoch, val_loss_epoch))\n",
    "\n",
    "    ############ save the best acc epoch ############\n",
    "    if val_acc_epoch >= max(val_acc_all):\n",
    "        torch.save(model, \"best_user.pt\")\n",
    "    \n",
    "train_endtime = time.time()\n",
    "\n",
    "print(\"Finish training! Using {:.4f} s:\".format(train_endtime - train_startime))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start to test \n",
    "\n",
    "We prepare our best weight file here. (named as \"best.pt\")\n",
    "The test accuracy based on our weight file is around 80%. \n",
    "\n",
    "You can also set your criterion to select your best weight, and test here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load default best weight\n",
      "Test Accuracy: 0.8222\n",
      "F1 score: 0.809913\n",
      "Test time: 0.3301 s\n",
      "Confusion matrix:\n",
      "[[35  0  9  0  0  0  0  0  1]\n",
      " [ 0 45  0  0  0  0  0  0  0]\n",
      " [ 7  0 37  0  0  0  0  0  0]\n",
      " [ 0  0  0 70  8  3  0  0  0]\n",
      " [ 0  0  0  2 29  1  0  0  0]\n",
      " [ 1  0  0 11  0 49  5  0  4]\n",
      " [ 0  0  0  2  0 11 14  0  0]\n",
      " [ 0  0  0  0  0  0  0 41  0]\n",
      " [ 0  0  0  4  0  7  0  0 19]]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"best_user.pt\"):\n",
    "    filename = \"best_default.pt\"\n",
    "    print(\"Load default best weight\")\n",
    "else:\n",
    "    filename=\"best_user.pt\"\n",
    "    print(\"Load user best weight\")\n",
    "\n",
    "\n",
    "model = torch.load(filename)  # read the best weight\n",
    "model.eval()    \n",
    "\n",
    "# print(model)\n",
    "\n",
    "test_acc_list = [] # list for storing the acc from each graph\n",
    "pre, gt = [], []\n",
    "\n",
    "\n",
    "test_startime = time.time()\n",
    "\n",
    "for batch, subgraph in enumerate(test_dataloader):\n",
    "    subgraph = subgraph.to(device)\n",
    "    # subgraph = dgl.add_self_loop(subgraph)\n",
    "\n",
    "    nfeat = subgraph.ndata['feat'].float()\n",
    "    efeat = subgraph.edata['relation'].float()\n",
    "\n",
    "    acc, _, _, _, one_pre, one_gt = evalEdge(model, nfeat, efeat, \\\n",
    "        subgraph, subgraph.ndata['label'], n_classes)\n",
    "\n",
    "    test_acc_list.append(acc)\n",
    "    pre.extend(one_pre)\n",
    "    gt.extend(one_gt)\n",
    "\n",
    "\n",
    "\n",
    "test_time = time.time() - test_startime\n",
    "\n",
    "test_acc = np.array(test_acc_list).mean()\n",
    "\n",
    "cm = confusion_matrix(gt, pre)  # confusion matrix, default function from scikit-learn\n",
    "\n",
    "f1 = f1_score(gt, pre, average='macro') # f1, default function from scikit-learn\n",
    "\n",
    "print(\"Test Accuracy: {:.4f}\".format(test_acc))\n",
    "\n",
    "print(\"F1 score: {:4f}\".format(f1))\n",
    "\n",
    "print(\"Test time: {:.4f} s\".format(test_time))\n",
    "\n",
    "print(f\"Confusion matrix:\\n{cm}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e70b37dd0163f83df31645134ef7e0128d64ec21afc2168ece8556baa31dc63"
  },
  "kernelspec": {
   "display_name": "Python 3.6.12 64-bit ('gnn': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
